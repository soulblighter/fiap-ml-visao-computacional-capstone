{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vusch4iBL-Mp"
   },
   "source": [
    "# MBA FIAP Inteligência Artificial & Machine Learning\n",
    "\n",
    "## Visão Computacional: Auditoria Automática de Vídeo Baseada em Modelos de Deep-Learning\n",
    "\n",
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versão local deste notebook, sem o sufixo ```-collab```.\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas. Por meio uma trilha guiada para construir uma aplicação que tem por objetivo analisar imagens e extrair uma série de informações que serão utilizadas para compor uma análise de imagens e vídeos afim de construir uma forma de auditoria automatizada baseado em modelos de inteligência artificial.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-capstone/blob/master/projeto/imagens-aux/example.png?raw=1\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
    "\n",
    "| Nome dos Integrantes     | RM            | Turma |\n",
    "| :----------------------- | :------------- | :-----: |\n",
    "| Júlio Cézar Moreira Menezes | RM 336389      | `10IA` |\n",
    "| Robson Ribeiro de Souza     | RM 335794      | `10IA` |\n",
    "| Rodrigo Hokamura            | RM 335820      | `10IA` |\n",
    "\n",
    "Por ser um projeto guiado, fique atento quando houver as marcações **Implementação** indica que é necessário realizar alguma implementação em Python no bloco a seguir onde há a inscrição ```## IMPLEMENTAR``` e **Resposta** indica que é esperado uma resposta objetiva relacionado a algum questionamento. \n",
    "\n",
    "**Cada grupo pode utilizar nas respostas objetivas quaisquer itens necessários que enriqueçam seu ponto vista, como gráficos, fotos e, até mesmo, trechos de código-fonte.**\n",
    "\n",
    "Pode-se utilizar quantos blocos forem necessários para realizar determinadas implementações ou utilizá-las para justificar as respostas. Não é obrigatório utilizar somente o bloco indicado.\n",
    "\n",
    "Ao final não se esqueça de subir os arquivos do projeto nas contas do GitHub de cada membro, ou subir na do representante do grupo e os membros realizarem o fork do projeto.\n",
    "\n",
    "A avaliação terá mais ênfase nos seguintes tópicos de desenvolvimento do projeto:\n",
    " \n",
    "1. __Modelo de identificação de idades__\n",
    "2. __Modelo de identificação de gênero__\n",
    "3. __Extração de faces (região de interesse)__\n",
    "4. __Análise de vídeo e extração de objetos__\n",
    "5. __Conclusões Finais__\n",
    "\n",
    "Para cada item haverá uma métrica de sucesso a ser perseguida e recomendações de como atingi-la. De todo modo os grupos terão liberdade para propor abordagem _ligeiramente_ diferentes para atinger as mesmas métricas. Apenas fica vetado o uso de APIs baseadas em cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMfP8XkpL-Mq"
   },
   "source": [
    "### 1.1 Componentes obrigatórios\n",
    "\n",
    "Todas as bibliotecas já estão instaladas no Google Colab.\n",
    "\n",
    "* Keras\n",
    "* Scikit-learn\n",
    "* Seaborn\n",
    "* DLib\n",
    "* OpenCv\n",
    "* Pandas\n",
    "* Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create -n tf python=3.6.9\n",
    "conda install -c conda-forge dlib\n",
    "conda install -c conda-forge matplotlib\n",
    "conda install -c conda-forge scikit-learn==0.23.1\n",
    "conda install -c conda-forge seaborn\n",
    "conda install -c conda-forge tqdm\n",
    "conda install -c conda-forge Pillow\n",
    "conda install -c conda-forge numpy\n",
    "conda install -c conda-forge opencv\n",
    "conda install -c conda-forge tensorflow-gpu\n",
    "conda install -c conda-forge jupyter\n",
    "conda activate tf\n",
    "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8889  --NotebookApp.port_retries=0  --no-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "laxd85P2H5ba",
    "outputId": "41bf1704-7902-4cb2-c867-f37bbfcabbc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzwg8bl7L-Ms"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#Exibição na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import pandas as pd\n",
    "import dlib\n",
    "\n",
    "import ast \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, ZeroPadding2D, Convolution2D, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XsjSulZ_L-Mw"
   },
   "source": [
    "### 1.1.1 Utilizando Google Drive\n",
    "\n",
    "Se usar o Google Drive para armazenar as imagens utilize o comando abaixo para montar seu drive.\n",
    "Você pode navegar pelas pastas pelo painel ao lado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cN4oOElL-Mx"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd \"/content/drive/My Drive/Colab Notebooks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXMD1GhFL-M0"
   },
   "source": [
    "### 1.1.2 Utilizando o armazenamento efêmero\n",
    "\n",
    "Se optar pelo armazenamento efêmero você precisa enviar os arquivos de imagens (arrastar e soltar) para as pastas que deverão ser criadas na estrutura do Colab e nas pastas que serão criadas no clone do repositório."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJA99zMGL-M1"
   },
   "source": [
    "### 1.1.3 Materiais de suporte e conjunto de dados\n",
    "\n",
    "As imagens e vídeos que serão utilizados no projeto estão no repositório. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5CT6637L-M1"
   },
   "outputs": [],
   "source": [
    "#!git clone  --bare https://github.com/soulblighter/fiap-ml-visao-computacional-capstone.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tgxdpkNWQcCo",
    "outputId": "4bd0ffb1-2b32-43d4-8817-eeee9469c099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Julio\\Desktop\\FIAP\\VisaoComputacional\\fiap-ml-visao-computacional-capstone\\projeto\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Julio\\Desktop\\FIAP\\VisaoComputacional\\fiap-ml-visao-computacional-capstone\\projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jGWbGU34L-M4",
    "outputId": "78df023e-f67d-42d1-d04d-8bf648f64482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] O sistema não pode encontrar o caminho especificado: 'fiap-ml-visao-computacional-capstone/projeto/'\n",
      "C:\\Users\\Julio\\Desktop\\FIAP\\VisaoComputacional\\fiap-ml-visao-computacional-capstone\\projeto\n"
     ]
    }
   ],
   "source": [
    "%cd fiap-ml-visao-computacional-capstone/projeto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIJWl-4_L-M7"
   },
   "source": [
    "## 1.2 O problema\n",
    "\n",
    "Atualmente, com a tecnologia disponível e acessível, sobretudo na obtenção de imagens e vídeos e em seu processamento, permite apoiar com mais ênfase atividades de auditoria e investigação de forma automatizada, sem a necessidade de uma pessoa realizar tais análises e ainda com maior acurácia.\n",
    "\n",
    "O uso de tecnologias relacionadas a visão computacional pode contribuir para tornar mais eficiente investigações de pessoas e objetos baseadas em suas características e em seu perfil, além de tornar esta atividade mais rápida e completa.\n",
    "\n",
    "A proposta deste projeto é apoiar uma aplicação de auditoria e investigação para automatizar a busca por regiões de interesse (objetos e determinadas pessoas) com as seguintes finalidades:\n",
    "\n",
    "* Encontrar pessoas do gênero masculino com mais de 45 anos de idade\n",
    "* Encontrar pessoas do gênero femimino com menos de 45 anos de idade\n",
    "* Encontrar objetos relacionados a informática, como computadores e telefones\n",
    "\n",
    "Uma busca manual em vídeos de vigilância é bem onerosa e pode deixar passar evidências importantes em processos investigativos.\n",
    "\n",
    "Ainda assim, a análise humana é realizada em último caso para decidir, se dentre as evidências coletadas, quais devem seguir para investigação mais apurada e quais não. A tarefa humana é mais de validação do que exploração.\n",
    "\n",
    "Para alcançar este objetivo iremos construir uma aplicação capaz de analisar um vídeo específico de um escritórioo, baseado na série de TV [_The Office_](https://pt.wikipedia.org/wiki/The_Office). A partir dele e de modelos de classificação de imagens, iremos coletar e armazenar imagens das regiões de interesse citadas para posterior análise de investigações.\n",
    "\n",
    "Deste modo, conforme já apresentado, seguiremos com o seguinte roteiro:\n",
    "\n",
    "* Desenvolver e construir um classificador de idade\n",
    "* Desenvolver e construir um classificador de gênero\n",
    "* Eleger a melhor forma de segmentar uma imagem de face\n",
    "* Configurar um modelo de detecção de objetos\n",
    "* Analisar um vídeo e extrair as regiões de interesse\n",
    "\n",
    "Por fim, deverá se realizado uma conclusão deste estudo, apresentando como foi a realização deste processo, pontos de acerto, pontos de melhoria e como poderia ser feito para aperfeiçoar os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJJAP-c5L-M7"
   },
   "source": [
    "## 2. Construção de modelos em redes neurais profundas\n",
    "\n",
    "Nesta primeira parte, iremos construir um modelo baseado em redes neurais profundas (_deep learning_) capaz de identificar, a partir de uma imagem, qual é a idade da pessoa.\n",
    "\n",
    "Este tipo de classificador requer um mapeamento mais profundo de cada imagem além de ser necessário um número considerável de imagens para cada idade ou faixa de idade.\n",
    "\n",
    "Devido a necessidade de um número alto de imagens, vamos utilizar o dataset [IMDB-WIKI – 500k+ face images with age and gender labels](https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki) que foi utilizado no desafio _DEX: Deep EXpectation of apparent age from a single image_.\n",
    "\n",
    "Também foi utilizado dois excelentes artigos de Sefik Ilkin Serengil, que podem ser acessados [aqui](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras) e [aqui](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras). Esses artigos demonstram a aplicação do VGGFace para as tarefas de reconhecimento de idade e gênero. Artigos que foram ligeiramente adaptados e utilizados para o desafio.\n",
    "\n",
    "As imagens estão disponíveis na pasta ```imagens```.\n",
    "\n",
    "O arquivo ```age-faces-dataset.csv```, na pasta ```csv``` possui a relação de cada sujeito, contendo sua idade, localização da face, idade e referência da imagem, gênero, dentre outros campos. Com esta referência é possível associar determinado sujeito com sua face.\n",
    "\n",
    "No conjunto de dados, a representação do gênero masculino é codificada com o valor *1* e o gênero feminino com o valor *0*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z65EbwqFL-M8"
   },
   "source": [
    "Abra o conjunto de dados utilizando o _Pandas_. Utilize o método ```read_csv```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36DSltUVL-M8"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "df = pd.read_csv(\"csv/age-faces-dataset.csv\" , delimiter=\",\" , encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6uhYBo8kL-M_"
   },
   "source": [
    "Execute o comando abaixo para apresentação de uma amostra do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "d57dIpRpL-NA",
    "outputId": "2cc63e3d-1d8e-4beb-f09f-c19df46e0e8a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>723671</td>\n",
       "      <td>2009</td>\n",
       "      <td>['17/10000217_1981-05-05_2009.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Sami Jauhojärvi']</td>\n",
       "      <td>[[111.29109473 111.29109473 252.66993082 252.6...</td>\n",
       "      <td>4.300962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1981</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>711677</td>\n",
       "      <td>2008</td>\n",
       "      <td>['12/100012_1948-07-03_2008.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Marc Okrand']</td>\n",
       "      <td>[[113.52 169.84 366.08 422.4 ]]</td>\n",
       "      <td>4.329329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>720044</td>\n",
       "      <td>2012</td>\n",
       "      <td>['16/10002116_1971-05-31_2012.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Diana Damrau']</td>\n",
       "      <td>[[171.61031405  75.5745124  266.76611571 170.7...</td>\n",
       "      <td>3.408442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1971</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>716189</td>\n",
       "      <td>2012</td>\n",
       "      <td>['02/10002702_1960-11-09_2012.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Krista Tippett']</td>\n",
       "      <td>[[274.7656324   57.77009008 376.88699455 159.8...</td>\n",
       "      <td>4.748056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1960</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>707745</td>\n",
       "      <td>1971</td>\n",
       "      <td>['41/10003541_1937-09-27_1971.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Bernie Whitebear']</td>\n",
       "      <td>[[ 79.35580189  26.65993396 197.60950472 144.9...</td>\n",
       "      <td>4.184828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1937</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     dob  photo_taken                            full_path  \\\n",
       "0           0  723671         2009  ['17/10000217_1981-05-05_2009.jpg']   \n",
       "1           2  711677         2008    ['12/100012_1948-07-03_2008.jpg']   \n",
       "2           4  720044         2012  ['16/10002116_1971-05-31_2012.jpg']   \n",
       "3           5  716189         2012  ['02/10002702_1960-11-09_2012.jpg']   \n",
       "4           6  707745         1971  ['41/10003541_1937-09-27_1971.jpg']   \n",
       "\n",
       "   gender                  name  \\\n",
       "0     1.0   ['Sami Jauhojärvi']   \n",
       "1     1.0       ['Marc Okrand']   \n",
       "2     0.0      ['Diana Damrau']   \n",
       "3     0.0    ['Krista Tippett']   \n",
       "4     1.0  ['Bernie Whitebear']   \n",
       "\n",
       "                                       face_location  face_score  \\\n",
       "0  [[111.29109473 111.29109473 252.66993082 252.6...    4.300962   \n",
       "1                    [[113.52 169.84 366.08 422.4 ]]    4.329329   \n",
       "2  [[171.61031405  75.5745124  266.76611571 170.7...    3.408442   \n",
       "3  [[274.7656324   57.77009008 376.88699455 159.8...    4.748056   \n",
       "4  [[ 79.35580189  26.65993396 197.60950472 144.9...    4.184828   \n",
       "\n",
       "   second_face_score  date_of_birth  age  \n",
       "0                NaN           1981   28  \n",
       "1                NaN           1948   60  \n",
       "2                NaN           1971   41  \n",
       "3                NaN           1960   52  \n",
       "4                NaN           1937   34  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqT9_30tL-NE"
   },
   "source": [
    "### 2.1 Modelo de predição de idade\n",
    "\n",
    "Antes de começarmos, avalie o conjunto de dados _Pandas_ e verifique se existe alguma otimização.\n",
    "Sugerimos verificar e filtrar somente sujeitos com idade maior do que 0 e menor do que 100.\n",
    "\n",
    "Analise o histograma e investigue com filtros do _Pandas_ para avaliar se existe necessidade de realização de algum _data cleasing_ no dataset, de acordo com a sugestão pedida.\n",
    "\n",
    "Havendo necessidade de _data cleasing_ realize os devidos ajustes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CgBEmmDjL-NF"
   },
   "source": [
    "Vamos investigar a distribuição de idades pelo histograma.\n",
    "Execute o comando abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "FOPT8kB8L-NG",
    "outputId": "fa70817c-e5a4-4d41-8a6d-3756c40982ec"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histograma_idade = df['age'].hist(bins=df['age'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVW34ROhL-NJ"
   },
   "source": [
    "De acordo com os dados de idade, vamos buscar a existência de idades que não serão foco dos modelos, por exemplo, idades inferiores a 25 e superiores a 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "colab_type": "code",
    "id": "Hy0fLD9DL-NK",
    "outputId": "834f89af-b357-4e5b-dbdf-3f7e2dcc0201"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>725873</td>\n",
       "      <td>2010</td>\n",
       "      <td>['82/10004882_1987-05-16_2010.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Viktor Pfeifer']</td>\n",
       "      <td>[[ 75.258  96.57  122.544 143.856]]</td>\n",
       "      <td>3.975478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28</td>\n",
       "      <td>714561</td>\n",
       "      <td>1969</td>\n",
       "      <td>['45/10013345_1956-05-26_1969.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Mehdi El Glaoui']</td>\n",
       "      <td>[[138.11816489 160.99119237 343.09541222 365.9...</td>\n",
       "      <td>4.598561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1956</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>36</td>\n",
       "      <td>726136</td>\n",
       "      <td>2012</td>\n",
       "      <td>['25/10016225_1988-02-03_2012.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Gregory van der Wiel']</td>\n",
       "      <td>[[249.85666584 105.28608568 321.26795593 176.6...</td>\n",
       "      <td>3.097300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1988</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>110</td>\n",
       "      <td>727835</td>\n",
       "      <td>2010</td>\n",
       "      <td>['79/10055379_1992-09-28_2010.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Keir Gilchrist']</td>\n",
       "      <td>[[ 61.24228141 121.60456282 331.99254776 392.3...</td>\n",
       "      <td>4.015771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>125</td>\n",
       "      <td>725376</td>\n",
       "      <td>2009</td>\n",
       "      <td>['03/10064003_1986-01-04_2009.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Charlyne Yi']</td>\n",
       "      <td>[[192.95160331  75.5745124  288.10740496 170.7...</td>\n",
       "      <td>3.722847</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22107</th>\n",
       "      <td>62207</td>\n",
       "      <td>725752</td>\n",
       "      <td>2008</td>\n",
       "      <td>['54/4998554_1987-01-15_2008.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Kelly Kelly']</td>\n",
       "      <td>[[ 29.04 169.84 281.6  422.4 ]]</td>\n",
       "      <td>3.881690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1987</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22108</th>\n",
       "      <td>62208</td>\n",
       "      <td>712766</td>\n",
       "      <td>1971</td>\n",
       "      <td>['60/499860_1951-06-27_1971.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Ulf Andersson']</td>\n",
       "      <td>[[151.78570353 151.78570353 286.7208367  286.7...</td>\n",
       "      <td>3.088425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1951</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22131</th>\n",
       "      <td>62276</td>\n",
       "      <td>725506</td>\n",
       "      <td>2008</td>\n",
       "      <td>['86/7998386_1986-05-14_2008.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Marco Motta']</td>\n",
       "      <td>[[ 92.85800375  43.13215558 156.28123712 106.5...</td>\n",
       "      <td>4.072333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22141</th>\n",
       "      <td>62305</td>\n",
       "      <td>727636</td>\n",
       "      <td>2013</td>\n",
       "      <td>['02/9994102_1992-03-13_2013.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['George MacKay']</td>\n",
       "      <td>[[115.24513741 160.99119237 320.22238474 365.9...</td>\n",
       "      <td>5.978161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22149</th>\n",
       "      <td>62318</td>\n",
       "      <td>708202</td>\n",
       "      <td>1962</td>\n",
       "      <td>['17/9996817_1938-12-28_1962.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Pachín']</td>\n",
       "      <td>[[ 94.17 117.53 303.68 327.04]]</td>\n",
       "      <td>3.559782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1938</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4489 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     dob  photo_taken                            full_path  \\\n",
       "7              12  725873         2010  ['82/10004882_1987-05-16_2010.jpg']   \n",
       "15             28  714561         1969  ['45/10013345_1956-05-26_1969.jpg']   \n",
       "19             36  726136         2012  ['25/10016225_1988-02-03_2012.jpg']   \n",
       "43            110  727835         2010  ['79/10055379_1992-09-28_2010.jpg']   \n",
       "45            125  725376         2009  ['03/10064003_1986-01-04_2009.jpg']   \n",
       "...           ...     ...          ...                                  ...   \n",
       "22107       62207  725752         2008   ['54/4998554_1987-01-15_2008.jpg']   \n",
       "22108       62208  712766         1971    ['60/499860_1951-06-27_1971.jpg']   \n",
       "22131       62276  725506         2008   ['86/7998386_1986-05-14_2008.jpg']   \n",
       "22141       62305  727636         2013   ['02/9994102_1992-03-13_2013.jpg']   \n",
       "22149       62318  708202         1962   ['17/9996817_1938-12-28_1962.jpg']   \n",
       "\n",
       "       gender                      name  \\\n",
       "7         1.0        ['Viktor Pfeifer']   \n",
       "15        1.0       ['Mehdi El Glaoui']   \n",
       "19        1.0  ['Gregory van der Wiel']   \n",
       "43        1.0        ['Keir Gilchrist']   \n",
       "45        0.0           ['Charlyne Yi']   \n",
       "...       ...                       ...   \n",
       "22107     0.0           ['Kelly Kelly']   \n",
       "22108     1.0         ['Ulf Andersson']   \n",
       "22131     1.0           ['Marco Motta']   \n",
       "22141     1.0         ['George MacKay']   \n",
       "22149     1.0                ['Pachín']   \n",
       "\n",
       "                                           face_location  face_score  \\\n",
       "7                    [[ 75.258  96.57  122.544 143.856]]    3.975478   \n",
       "15     [[138.11816489 160.99119237 343.09541222 365.9...    4.598561   \n",
       "19     [[249.85666584 105.28608568 321.26795593 176.6...    3.097300   \n",
       "43     [[ 61.24228141 121.60456282 331.99254776 392.3...    4.015771   \n",
       "45     [[192.95160331  75.5745124  288.10740496 170.7...    3.722847   \n",
       "...                                                  ...         ...   \n",
       "22107                    [[ 29.04 169.84 281.6  422.4 ]]    3.881690   \n",
       "22108  [[151.78570353 151.78570353 286.7208367  286.7...    3.088425   \n",
       "22131  [[ 92.85800375  43.13215558 156.28123712 106.5...    4.072333   \n",
       "22141  [[115.24513741 160.99119237 320.22238474 365.9...    5.978161   \n",
       "22149                    [[ 94.17 117.53 303.68 327.04]]    3.559782   \n",
       "\n",
       "       second_face_score  date_of_birth  age  \n",
       "7                    NaN           1987   23  \n",
       "15                   NaN           1956   13  \n",
       "19                   NaN           1988   24  \n",
       "43                   NaN           1992   18  \n",
       "45                   NaN           1986   23  \n",
       "...                  ...            ...  ...  \n",
       "22107                NaN           1987   21  \n",
       "22108                NaN           1951   20  \n",
       "22131                NaN           1986   22  \n",
       "22141                NaN           1992   21  \n",
       "22149                NaN           1938   24  \n",
       "\n",
       "[4489 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age'] < 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyi_wBGyL-NO"
   },
   "source": [
    "Agora idades maiores do que 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889
    },
    "colab_type": "code",
    "id": "zGIlDdn5L-NO",
    "outputId": "7a782a97-646c-4386-8d84-ae0c271d4eb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19</td>\n",
       "      <td>705106</td>\n",
       "      <td>2008</td>\n",
       "      <td>['81/1000781_1930-07-07_2008.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Theodore Edgar McCarrick']</td>\n",
       "      <td>[[126.47169334  28.58548741 251.70738669 153.8...</td>\n",
       "      <td>3.680456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>65</td>\n",
       "      <td>710562</td>\n",
       "      <td>2012</td>\n",
       "      <td>['59/10033359_1945-06-14_2012.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Bruce Degen']</td>\n",
       "      <td>[[123.45351931  98.93881545 343.20585407 318.6...</td>\n",
       "      <td>4.112618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1945</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>101</td>\n",
       "      <td>690933</td>\n",
       "      <td>1958</td>\n",
       "      <td>['00/10049200_1891-09-16_1958.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Isabel Jeans']</td>\n",
       "      <td>[[126.86918793 111.11053944 267.89702434 252.1...</td>\n",
       "      <td>5.254819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1891</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>104</td>\n",
       "      <td>707724</td>\n",
       "      <td>2010</td>\n",
       "      <td>['79/1005079_1937-09-06_2010.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Jo Anne Worley']</td>\n",
       "      <td>[[ 85.14 106.26 274.56 295.68]]</td>\n",
       "      <td>3.334351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1937</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>136</td>\n",
       "      <td>706891</td>\n",
       "      <td>2005</td>\n",
       "      <td>['92/1007192_1935-05-27_2005.jpg']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['Lee Meriwether']</td>\n",
       "      <td>[[176.72563637  40.67491736 248.09248761 112.0...</td>\n",
       "      <td>3.706718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1935</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22120</th>\n",
       "      <td>62249</td>\n",
       "      <td>704118</td>\n",
       "      <td>2013</td>\n",
       "      <td>['07/699407_1927-10-23_2013.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Barron Hilton']</td>\n",
       "      <td>[[ 98.93881545 147.96822317 318.69115021 367.7...</td>\n",
       "      <td>3.629797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1927</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22124</th>\n",
       "      <td>62262</td>\n",
       "      <td>710677</td>\n",
       "      <td>2011</td>\n",
       "      <td>['46/7991846_1945-10-07_2011.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Jean-Luc Thérier']</td>\n",
       "      <td>[[ 57.38109083  49.23522072 130.33392191 122.1...</td>\n",
       "      <td>4.203977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1945</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22133</th>\n",
       "      <td>62281</td>\n",
       "      <td>712547</td>\n",
       "      <td>2011</td>\n",
       "      <td>['55/7999955_1950-11-20_2011.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Rudy Jaramillo']</td>\n",
       "      <td>[[112.2304721   67.65828326 312.00532189 267.4...</td>\n",
       "      <td>3.411976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22145</th>\n",
       "      <td>62312</td>\n",
       "      <td>704906</td>\n",
       "      <td>2008</td>\n",
       "      <td>['83/9996683_1929-12-19_2008.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Gianfranco Bedin']</td>\n",
       "      <td>[[ 41.17331311 102.15928277 223.61522208 284.6...</td>\n",
       "      <td>3.245726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1929</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22146</th>\n",
       "      <td>62313</td>\n",
       "      <td>706289</td>\n",
       "      <td>2008</td>\n",
       "      <td>['63/9996763_1933-10-02_2008.jpg']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['Giuliano Sarti']</td>\n",
       "      <td>[[ 53.39958974  88.65664956 211.54235894 246.7...</td>\n",
       "      <td>4.047994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1933</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2384 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     dob  photo_taken                            full_path  \\\n",
       "10             19  705106         2008   ['81/1000781_1930-07-07_2008.jpg']   \n",
       "28             65  710562         2012  ['59/10033359_1945-06-14_2012.jpg']   \n",
       "38            101  690933         1958  ['00/10049200_1891-09-16_1958.jpg']   \n",
       "39            104  707724         2010   ['79/1005079_1937-09-06_2010.jpg']   \n",
       "50            136  706891         2005   ['92/1007192_1935-05-27_2005.jpg']   \n",
       "...           ...     ...          ...                                  ...   \n",
       "22120       62249  704118         2013    ['07/699407_1927-10-23_2013.jpg']   \n",
       "22124       62262  710677         2011   ['46/7991846_1945-10-07_2011.jpg']   \n",
       "22133       62281  712547         2011   ['55/7999955_1950-11-20_2011.jpg']   \n",
       "22145       62312  704906         2008   ['83/9996683_1929-12-19_2008.jpg']   \n",
       "22146       62313  706289         2008   ['63/9996763_1933-10-02_2008.jpg']   \n",
       "\n",
       "       gender                          name  \\\n",
       "10        1.0  ['Theodore Edgar McCarrick']   \n",
       "28        1.0               ['Bruce Degen']   \n",
       "38        0.0              ['Isabel Jeans']   \n",
       "39        0.0            ['Jo Anne Worley']   \n",
       "50        0.0            ['Lee Meriwether']   \n",
       "...       ...                           ...   \n",
       "22120     1.0             ['Barron Hilton']   \n",
       "22124     1.0          ['Jean-Luc Thérier']   \n",
       "22133     1.0            ['Rudy Jaramillo']   \n",
       "22145     1.0          ['Gianfranco Bedin']   \n",
       "22146     1.0            ['Giuliano Sarti']   \n",
       "\n",
       "                                           face_location  face_score  \\\n",
       "10     [[126.47169334  28.58548741 251.70738669 153.8...    3.680456   \n",
       "28     [[123.45351931  98.93881545 343.20585407 318.6...    4.112618   \n",
       "38     [[126.86918793 111.11053944 267.89702434 252.1...    5.254819   \n",
       "39                       [[ 85.14 106.26 274.56 295.68]]    3.334351   \n",
       "50     [[176.72563637  40.67491736 248.09248761 112.0...    3.706718   \n",
       "...                                                  ...         ...   \n",
       "22120  [[ 98.93881545 147.96822317 318.69115021 367.7...    3.629797   \n",
       "22124  [[ 57.38109083  49.23522072 130.33392191 122.1...    4.203977   \n",
       "22133  [[112.2304721   67.65828326 312.00532189 267.4...    3.411976   \n",
       "22145  [[ 41.17331311 102.15928277 223.61522208 284.6...    3.245726   \n",
       "22146  [[ 53.39958974  88.65664956 211.54235894 246.7...    4.047994   \n",
       "\n",
       "       second_face_score  date_of_birth  age  \n",
       "10                   NaN           1930   78  \n",
       "28                   NaN           1945   67  \n",
       "38                   NaN           1891   67  \n",
       "39                   NaN           1937   73  \n",
       "50                   NaN           1935   70  \n",
       "...                  ...            ...  ...  \n",
       "22120                NaN           1927   86  \n",
       "22124                NaN           1945   66  \n",
       "22133                NaN           1950   61  \n",
       "22145                NaN           1929   79  \n",
       "22146                NaN           1933   75  \n",
       "\n",
       "[2384 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age'] > 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xelpaXJQL-NS"
   },
   "source": [
    "Diante dos dados apresentados, realize a limpeza dos registros com idades inválidas ou de não interesse do projeto.\n",
    "Para realizar a limpeza utilize da seguinte forma:\n",
    "\n",
    "```python\n",
    "df = df[df['age'] <= idade_limite_superior]\n",
    "df = df[df['age'] > idade_limite_inferior]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0R5N-pnKL-NS"
   },
   "source": [
    "Agora implemente a limpeza dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q57HAOA-L-NT"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "df = df[df['age'] <= 100]\n",
    "df = df[df['age'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fv96yQNbL-NW"
   },
   "source": [
    "Vamos executar novamente os comandos abaixo para ter certeza que os dados foram limpos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mh7XRZCeL-NW",
    "outputId": "4f5d6ca3-aaa9-4676-d9c3-8e8d11348d9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['age'] > 100]), len(df[df['age'] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sHuLqbbL-NZ"
   },
   "source": [
    "Se o resultado obtido foi 0 para ambos os filtros (exemplo (0,0)) signfica que estamos prontos para avançar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rh66pNfJL-NZ"
   },
   "source": [
    "### 2.2 Transfer learning\n",
    "\n",
    "A técnica de _transfer learning_ é particularmente útil ao combinar modelos já validados em aplicações mais robustas, resultado de competições de grande porte.\n",
    "\n",
    "Um benchmark nesse campo aplicado a faces é o [VGGFace](http://www.robots.ox.ac.uk/~vgg/data/vgg_face) do grupo *VGG*, o Visual Geometry Group da Univerdade de Oxford.\n",
    "\n",
    "A arquitetura proposta pelo VGGFace é capaz de classificar faces com precisão acima de 90% em alguns trabalhos, como por exemplo [Zhang, Lingfeng & Kakadiaris, Ioannis. (2017). Local classifier chains for deep face recognition. 158-167](https://www.researchgate.net/publication/322872468_Local_classifier_chains_for_deep_face_recognition).\n",
    "\n",
    "Este modelo tem a seguinte arquitetura.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-capstone/blob/master/projeto/imagens-aux/vgg-face-architecture.jpg?raw=1\">\n",
    "</p>\n",
    "\n",
    "Observando a primeira camada, precisamos portanto padronizar as imagens de treinamento no tamanho 224 x 224 (comprimento x largura).\n",
    "\n",
    "Os grupos que decidirem criar modelos alternativos devem explicar qual arquitetura utiliza e seus motivadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCNVgfF7L-Na"
   },
   "source": [
    "### 2.3 Padronização de imagens\n",
    "\n",
    "O algoritmo abaixo normaliza as imagens para o tamanho de entrada do modelo VGGFace e também converte o valor em intenside de pixel de 0 a 255 para 0 a 1, com valores em pontos flutuantes.\n",
    "\n",
    "Valores em ponto flutuante são melhores para se trabalhar com convergência de modelos durante os cálculos entre as camadas das redes neurais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2WQh5vfL-Nb"
   },
   "source": [
    "> Atenção: para melhor controlar a quantidade de memória, opte por incluir mais ou menos imagens por classe. Abaixo a variável ```qtde_imagens_classe``` regula isso. Ambientes limitados do Colab a recomendação é no máximo 200. Fique a vontade para inclui mais, pois quanto maior o número de imagens, mais preciso é o modelo. Cada classe possui cerca de 600 imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Frj-3ivAL-Nc"
   },
   "outputs": [],
   "source": [
    "qtde_imagens_classe = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFrp5IQqL-Ne"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tamanho_imagem = (224, 224)\n",
    "faces = []\n",
    "idades = []\n",
    "generos = []\n",
    "faces_ages = {}\n",
    "\n",
    "def carregar_imagens_faces():\n",
    "    faces = []\n",
    "    idades = []\n",
    "    generos = []\n",
    "    faces_ages = {}\n",
    "    for index, row in tqdm(df[::-1].iterrows(), total=df.shape[0]):\n",
    "        if row[\"age\"] in faces_ages:\n",
    "          faces_ages[row[\"age\"]] += 1\n",
    "        else:\n",
    "          faces_ages[row[\"age\"]] = 1\n",
    "\n",
    "        if faces_ages[row[\"age\"]] > qtde_imagens_classe:\n",
    "          #df.drop(df.index[index],inplace=True)\n",
    "          continue\n",
    "\n",
    "        idades.append(row[\"age\"])\n",
    "        generos.append(row[\"gender\"])\n",
    "\n",
    "        image_face = image.load_img(\"imagens/%s\" % ast.literal_eval(row[\"full_path\"])[0], grayscale=False, target_size=tamanho_imagem)\n",
    "        image_array = image.img_to_array(image_face).reshape(1, -1)[0]\n",
    "        image_array /= 255\n",
    "        faces.append(image_array)\n",
    "    return faces, idades, generos, faces_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmZrbCtwL-Ni"
   },
   "source": [
    "Verifique a quantidade de imagens de faces obtidas pelo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "gO3PUl1HL-Nj",
    "outputId": "5456a796-7641-4724-c87b-f3fe89b268b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 22140/22140 [00:47<00:00, 468.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTAR\n",
    "faces, idades, generos, faces_ages = carregar_imagens_faces()\n",
    "qtd_faces = len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "55v-fGU7L-Nl",
    "outputId": "85acf0e3-1aa8-4110-c946-97d47a5ddbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O total de faces de imagens é de 17605.\n"
     ]
    }
   ],
   "source": [
    "print(\"O total de faces de imagens é de \" + str(qtd_faces) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_fnadmdL-No"
   },
   "source": [
    "### 2.4 Definição do modelo\n",
    "\n",
    "O próximo passo é a definição do número de classes. Como a definição de classes é _zero based_ sempre defina adicionando 1 ao valor final.\n",
    "\n",
    "Exemplo, se o número de classes compreender entre idades maior do que 0 e menor ou igual a 100, teremos 100 classes, logo o número de classes de idade é 101."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fxO5U_TEL-No"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "classes_idade = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8k2ruubL-Ns"
   },
   "source": [
    "Uma forma de categorizarmos as classes é utilizando o modelo _one hot encode_. Onde será expressado em um array com todas as classes. A classe correspondente a um determindo registro será armazenado como o valor 1 naquela classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVH5UVUpL-Nt"
   },
   "outputs": [],
   "source": [
    "idades_classes = keras.utils.to_categorical(idades, classes_idade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GiC0pQExL-Nv"
   },
   "source": [
    "Faça um teste do _one hot encoding_. Exiba os dados do registro de índice zero do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "3jn3rDxbL-Nw",
    "outputId": "277fd610-2531-4625-8d5d-db4ebce5599f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idades_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tQdSouPL-Ny"
   },
   "source": [
    "Em seguida, vamos adaptar o _shape_ dos dados no padrão que o framework Keras utiliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "qG4B3h7jL-Ny",
    "outputId": "46d72db2-63ea-495f-8d61-85354c84d945"
   },
   "outputs": [],
   "source": [
    "features_imagem = []\n",
    "\n",
    "features_imagem = np.array(faces)\n",
    "features_imagem = features_imagem.reshape(features_imagem.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWeqDMwlL-N1"
   },
   "source": [
    "Para liberar o uso de memória, vamos eliminando variáveis que não mais finalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vALu_GObL-N2"
   },
   "outputs": [],
   "source": [
    "faces = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cYMJUM5L-N5"
   },
   "source": [
    "### 2.5 Separação de dados de treinamento e validação\n",
    "\n",
    "Para que o modelo seja robusto, é importante validar um subconjunto de amostras que não participaram do treinamento inicial.\n",
    "\n",
    "Defina uma porcentagem, da base total, que deverá ser separa somente para a validação. Expresse o número na forma de número fracionário, por exemplo 40% equivale a 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGRf8FPlL-N6"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "porcentagem_validacao = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Taz5A583L-N8"
   },
   "source": [
    "Execute o método ```train_test_split``` para realizar a divisão de cada tipo de amostra, sendo:\n",
    "* ```treinamento_x``` contém os _features_ de treinamento\n",
    "* ```teste_x``` contém os _features_ de validação (também é chamado de teste)\n",
    "* ```treinamento_y``` contém as classes de treinamento\n",
    "* ```teste_y``` contém as classes de validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-86ljlCEL-N8"
   },
   "outputs": [],
   "source": [
    "treinamento_x, teste_x, treinamento_y, teste_y = train_test_split(features_imagem, \n",
    "                                                                  idades_classes, \n",
    "                                                                  test_size=porcentagem_validacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGSjj6S1L-N_"
   },
   "source": [
    "Liberando memória pois a variável ```features_imagem``` neste ponto não será mais utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "agjbp9mXL-N_"
   },
   "outputs": [],
   "source": [
    "features_imagem = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5GXG8yxyL-OB"
   },
   "source": [
    "### 2.6 Treinamento e validação\n",
    "\n",
    "O modelo VGGFace ainda não está por padrão na biblioteca Keras. Mesmo assim, como o artigo citado anteriormente possui a arquitetura definida, podemos implementá-la manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3D8v86bJL-OB"
   },
   "outputs": [],
   "source": [
    "#VGG-Face model\n",
    "modelo = Sequential()\n",
    "modelo.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "modelo.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "modelo.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "modelo.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "modelo.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(ZeroPadding2D((1,1)))\n",
    "modelo.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "modelo.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "modelo.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "modelo.add(Dropout(0.5))\n",
    "modelo.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "modelo.add(Dropout(0.5))\n",
    "modelo.add(Convolution2D(2622, (1, 1)))\n",
    "modelo.add(Flatten())\n",
    "modelo.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfdAAQfTL-OE"
   },
   "source": [
    "Os pesos já treinamentos podem ser baixados neste [link](https://drive.google.com/open?id=1xJY1mWULLOnK5GAGULcp_cgasVfy-e2j). Copie ele para a pasta ```pesos```.\n",
    "O arquivo de pesos é o mesmo utilizado neste [artigo](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_wvLPWPEJl2"
   },
   "outputs": [],
   "source": [
    "# modificação: os pesos foram adicionados no projeto como arquivos splitados\n",
    "# porque o github so permite arquivos de no máximo 100 MB\n",
    "# assim n eh necessario baixar o arquivo manualmente\n",
    "!cat pesos/vgg_face_weights.split.* > pesos/vgg_face_weights.h5\n",
    "!cat pesos/yolov3.split.* > pesos/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8Ax50uoL-OF"
   },
   "outputs": [],
   "source": [
    "modelo.load_weights(\"pesos/vgg_face_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVskowiML-OH"
   },
   "source": [
    "Assim como em qualquer modelo de *transfer learning*, precisamos adicionar na última camada as classes correspondentes. Deste modo, na saída da camada convulacional precisamos definir o número de classes. É o primeiro parâmetro do objeto ```Convolution2D```, neste caso incluíremos a variável que determina o número de classes ```classes_idade```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87AdvRnKL-OI"
   },
   "outputs": [],
   "source": [
    "for layer in modelo.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "saida_modelo = Sequential()\n",
    "\n",
    "saida_modelo = Convolution2D(classes_idade, (1, 1), name='predictions')(modelo.layers[-4].output)\n",
    "\n",
    "saida_modelo = Flatten()(saida_modelo)\n",
    "saida_modelo = Activation('softmax')(saida_modelo)\n",
    "modelo_idade = Model(inputs=modelo.input, outputs=saida_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMbtVQH-L-OK"
   },
   "source": [
    "Este tipo de treinamento possui muitas imagens. A etapa de treinamento pode demorar até mesmo mais do que 2 horas dependendo do tipo de computador utilizado.\n",
    "\n",
    "Apesar da demora, este desafio traz um desafio real de treinar um modelo de imagens com quantidade de exemplos adequado para as classificações requeridas.\n",
    "\n",
    "Sugiro que após o treinamento ser realizado, salvar os novos pesos e a rede para utilizar mais adiante nas inferências.\n",
    "\n",
    "Revise a arquitetura da rede com o comando a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hULTGI4sL-OL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_input (InputL [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "predictions (Conv2D)         (None, 1, 1, 101)         413797    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 101)               0         \n",
      "=================================================================\n",
      "Total params: 134,674,341\n",
      "Trainable params: 119,959,653\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo_idade.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cJU0MMRL-ON"
   },
   "source": [
    "O número de *épocas* é responsável por quantas vezes o modelo percorrerá o ciclo de _foward_ e _back_ _propagation_.\n",
    "\n",
    "Como este modelo possui muitas imagens, sugerimos um valor mínimo de 2 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMdRXQt9L-ON"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "numero_epocas = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saUft1R0L-OP"
   },
   "source": [
    "*Caso o modelo trave no treinamento, experimente diminuir o ```batch_size```.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bREw3tw0L-OQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10563 samples, validate on 7042 samples\n",
      "Epoch 1/5\n",
      "    2/10563 [..............................] - ETA: 3:14:15"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv2d/Conv2D (defined at <ipython-input-33-fda15603a6c3>:3) ]] [Op:__inference_distributed_function_2042]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-fda15603a6c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodelo_idade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m historico = modelo_idade.fit(x=treinamento_x, y=treinamento_y, \n\u001b[1;32m----> 3\u001b[1;33m                              validation_data=(teste_x, teste_y), epochs=numero_epocas, batch_size=2)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf2\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node model/conv2d/Conv2D (defined at <ipython-input-33-fda15603a6c3>:3) ]] [Op:__inference_distributed_function_2042]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "modelo_idade.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "historico = modelo_idade.fit(x=treinamento_x, y=treinamento_y, \n",
    "                             validation_data=(teste_x, teste_y), epochs=numero_epocas, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXnKk2nfL-OS"
   },
   "source": [
    "Armazene os valores do modelo e dos pesos para inferências futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrqffbRGL-OT"
   },
   "outputs": [],
   "source": [
    "# Salvando o modelo no formato HDf5\n",
    "modelo_idade.save(\"modelos/modelo_idade.h5\")\n",
    "modelo_idade.save_weights(\"pesos/modelo_idade_pesos.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ff3PDvfjL-OW"
   },
   "source": [
    "Os valores de erro e acurácia são importantes indicadores de desempenho da rede. Avaliar a tendência de subida ou descida destes parâmetros é essencial para localizar, por exemplo, prooblemas de _overfitting_ e _underfitting_. \n",
    "Com o suporte dos gráficos avalie a tendência do modelo ao logo das épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLRxWsKLL-OW"
   },
   "outputs": [],
   "source": [
    "# Para deixar no formato do Seaborn os gráficos do Pyplot\n",
    "sns.set()\n",
    "\n",
    "acc_var = 'acc'\n",
    "valacc_var = 'val_acc'\n",
    "# in some versions is 'accuracy' or 'acc'\n",
    "if 'accuracy' in historico.history:\n",
    "    acc_var = 'accuracy'\n",
    "    valacc_var = 'val_accuracy'\n",
    "\n",
    "\n",
    "# Exibindo dados de Acurácia/Precisão\n",
    "plt.plot(historico.history[acc_var])\n",
    "plt.plot(historico.history[valacc_var])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Exibindo dados de Perda\n",
    "plt.plot(historico.history['loss'])\n",
    "plt.plot(historico.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWVjcCAaL-OY"
   },
   "source": [
    "**Pergunta:** Qual foi a tendência da função de erro e acurácia do modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WsKyFSCgL-OZ"
   },
   "source": [
    "**Resposta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVsV8uTyL-Oa"
   },
   "source": [
    "Os comandos abaixo são de referência para carregar um modelo para inferências futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai7lRsasL-Ob"
   },
   "outputs": [],
   "source": [
    "# carregando o modelo no formato HDf5\n",
    "modelo_idade = load_model(\"modelos/modelo_idade.h5\")  \n",
    "modelo_idade.load_weights(\"pesos/modelo_idade_pesos.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Vbmwe3WL-Oe"
   },
   "source": [
    "Este classificador tem uma particularidade diferente quando comparado com outros classificadores.\n",
    "Geralmente um classificador retorna, dado uma amostra, qual a classe mais próxima ou qual tem maior probabilidade estatística. Por exemplo, se fóssemos classificar um objeto, o retorno seria a classe correspondente àquele objeto com maior semelhança, mutualmente excludente, sendo uma única classe como resultado.\n",
    "\n",
    "Neste caso é diferente.\n",
    "\n",
    "Cada classe se refere a uma idade, logo, ao invés de nos basearmos na classe dominante, com maior probabilidade, podemos somar as probabilidades com cada classe e assim ter um valor de idade mais aproximado (veja na imagem abaixo do pipeline, item 5). \n",
    "\n",
    "É por essa razão que a acurácia do modelo, baseada somente na classe dominante é baixa.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-capstone/blob/master/projeto/imagens-aux/pipeline.png?raw=1\">\n",
    "</p>\n",
    "\n",
    "Os comandos abaixo contabilizam a predição baseada na soma das probabilidades de cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMX1wtByL-Oe"
   },
   "outputs": [],
   "source": [
    "predicoes_idade = modelo_idade.predict(teste_x)\n",
    "classes_idade_saida = np.array([i for i in range(0, classes_idade)])\n",
    "predicoes_agrupadas = np.sum(predicoes_idade * classes_idade_saida, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Swb_TKXIL-Oj"
   },
   "source": [
    "Deste modo, o valor da acurácia não é o melhor para avaliarmoso modelo. A uma outra forma de avaliação é utilizarmos o _Erro Médio Absoluto_.\n",
    "\n",
    "O uso do _Erro Médio Absoluto_ foi integralmente aplicado de acordo com este [artigo](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGcr6OV3L-Ok"
   },
   "outputs": [],
   "source": [
    "erro_medio_absoluto = 0\n",
    "atual_media = 0\n",
    "\n",
    "for i in range(0 ,predicoes_agrupadas.shape[0]):\n",
    "    predicao = int(predicoes_agrupadas[i])\n",
    "    atual = np.argmax(teste_y[i])\n",
    "    erro_abs = abs(predicao - atual)\n",
    "    atual_media = atual_media + atual\n",
    "    erro_medio_absoluto += erro_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xdRb_Q-L-Om"
   },
   "outputs": [],
   "source": [
    "erro_medio_absoluto /= predicoes_agrupadas.shape[0]\n",
    " \n",
    "print(\"Erro médio absoluto (+/-): \", erro_medio_absoluto, \" anos\")\n",
    "print(\"Exemplos analisados: \",predicoes_agrupadas.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jaBN5uoGL-Oq"
   },
   "source": [
    "> **Critério de sucesso:** com base nas amostras de teste, o erro absoluto precisa ser não mais do que (+/-) 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qC2Xr--L-Or"
   },
   "source": [
    "### 2.7 Avaliação final\n",
    "\n",
    "Vamos analisar um conjunto de 6 imagens e verificar a coerência do modelo.\n",
    "Antes precisamos contruir uma função, a ```carregarImagem``` para padronizar a imagem, redimensionando para o formato do modelo e normalizando a intensidade dos pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShWFzOU0L-Os"
   },
   "outputs": [],
   "source": [
    "def carregarImagem(caminho):\n",
    "    imagem = image.load_img(caminho, target_size=(224, 224))\n",
    "    imagem = image.img_to_array(imagem)\n",
    "    imagem = np.expand_dims(imagem, axis = 0)\n",
    "    imagem /= 255\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z99jD9Y4L-Ou"
   },
   "source": [
    "Para cada imagem de testes já separadas no diretório ```testes```, vamos padronizá-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKH2fX0-L-Ou"
   },
   "outputs": [],
   "source": [
    "caminho_imagem_1 = \"testes/teste-1.jpg\"\n",
    "imagem_1_pad = carregarImagem(caminho_imagem_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfODJBDfL-Ox"
   },
   "source": [
    "Executar a inferência para obter sua predição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uX64af84L-Oz"
   },
   "outputs": [],
   "source": [
    "predicao_1 = modelo_idade.predict(imagem_1_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jf2daI4pL-O1"
   },
   "source": [
    "E, finalmente, somar as classes para a classificação mais exata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSdYnPfBL-O2"
   },
   "outputs": [],
   "source": [
    "idade_agrupada_1 = np.round(np.sum(predicao_1 * classes_idade_saida, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoVHMHV8L-O4"
   },
   "source": [
    "A função abaixo é para exibirmos no gráfico. Como elas estão com tamanhos diferentes, vamos também redimensioná-las para o padrão do moodeo, apenas por convenção, poderia ser outro tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u70P5m4aL-O4"
   },
   "outputs": [],
   "source": [
    "imagem_1 = image.load_img(caminho_imagem_1, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jgozD2KL-O7"
   },
   "source": [
    "Agora, faremos para todas as outras imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJQTxtTCL-O8"
   },
   "outputs": [],
   "source": [
    "caminho_imagem_2 = \"testes/teste-2.png\"\n",
    "imagem_2_pad = carregarImagem(caminho_imagem_2)\n",
    "imagem_2 = image.load_img(caminho_imagem_2, target_size=(224, 224))\n",
    "predicao_2 = modelo_idade.predict(imagem_2_pad)\n",
    "idade_agrupada_2 = np.round(np.sum(predicao_2 * classes_idade_saida, axis = 1))\n",
    "\n",
    "caminho_imagem_3 = \"testes/teste-3.png\"\n",
    "imagem_3_pad = carregarImagem(caminho_imagem_3)\n",
    "imagem_3 = image.load_img(caminho_imagem_3, target_size=(224, 224))\n",
    "predicao_3 = modelo_idade.predict(imagem_3_pad)\n",
    "idade_agrupada_3 = np.round(np.sum(predicao_3 * classes_idade_saida, axis = 1))\n",
    "\n",
    "caminho_imagem_4 = \"testes/teste-4.png\"\n",
    "imagem_4_pad = carregarImagem(caminho_imagem_4)\n",
    "imagem_4 = image.load_img(caminho_imagem_4, target_size=(224, 224))\n",
    "predicao_4 = modelo_idade.predict(imagem_4_pad)\n",
    "idade_agrupada_4 = np.round(np.sum(predicao_4 * classes_idade_saida, axis = 1))\n",
    "\n",
    "caminho_imagem_5 = \"testes/teste-5.png\"\n",
    "imagem_5_pad = carregarImagem(caminho_imagem_5)\n",
    "imagem_5 = image.load_img(caminho_imagem_5, target_size=(224, 224))\n",
    "predicao_5 = modelo_idade.predict(imagem_5_pad)\n",
    "idade_agrupada_5 = np.round(np.sum(predicao_5 * classes_idade_saida, axis = 1))\n",
    "\n",
    "caminho_imagem_6 = \"testes/teste-6.png\"\n",
    "imagem_6_pad = carregarImagem(caminho_imagem_6)\n",
    "imagem_6 = image.load_img(caminho_imagem_6, target_size=(224, 224))\n",
    "predicao_6 = modelo_idade.predict(imagem_6_pad)\n",
    "idade_agrupada_6 = np.round(np.sum(predicao_6 * classes_idade_saida, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5x5BtleLL-PA"
   },
   "source": [
    "Exibindo as imagens.\n",
    "\n",
    "O resultado da idade é índice 0 do objeto ```idade_agrupada```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ryMZhepmL-PB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(231)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_1[0]))\n",
    "plt.imshow(imagem_1)\n",
    "plt.subplot(232)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_2[0]))\n",
    "plt.imshow(imagem_2)\n",
    "plt.subplot(233)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_3[0]))\n",
    "plt.imshow(imagem_3)\n",
    "plt.subplot(234)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_4[0]))\n",
    "plt.imshow(imagem_4)\n",
    "plt.subplot(235)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_5[0]))\n",
    "plt.imshow(imagem_5)\n",
    "plt.subplot(236)\n",
    "plt.title(\"Idade inferida \" + str(idade_agrupada_6[0]))\n",
    "plt.imshow(imagem_6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvgO8ph_L-PE"
   },
   "source": [
    "Analise se as idades estão coerentes com as imagens. Se estiver muito fora, avalie aumentar o número de épocas, por exemplo.\n",
    "\n",
    "Agora vamos implementar a função que retornará a idade a partir de uma imagem de entrada já padronizada (assuma que foi padronizada por uma função como a ```carregarImagem```. Assegure que a função retorne o valor da idade inferida em valor numérico, sem valor fracionário, somente inteiro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tUA4Y70L-PE"
   },
   "outputs": [],
   "source": [
    "def predizerIdade(imagem):\n",
    "    # IMPLEMENTAR\n",
    "    predicao = modelo_idade.predict(imagem)\n",
    "    return np.round(np.sum(predicao * classes_idade_saida, axis = 1)).item(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azkugC8KL-PK"
   },
   "source": [
    "Liberando memória de variáveis sem utilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiGjjapgL-PK"
   },
   "outputs": [],
   "source": [
    "treinamento_x, teste_x, treinamento_y, teste_y  = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "vOXdSGbJL-PO"
   },
   "source": [
    "## 3. Modelo de predição de gênero biológico\n",
    "\n",
    "A base utilizada será a mesma que aplicamos no modeo de idade. Nesse o caso o modelo terá uma tarefa mais fácil, pois ao invés de aproximidamente 100 classes vamos ter somente 2 classes, uma cada definir o gênero masculino e outra para o gênero feminino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkYoafeLL-PO"
   },
   "source": [
    "### 3.1 Definição do modelo\n",
    "Vamos começar definindo o número de classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kZVWDn_L-PP"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "num_classes_genero = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Xc5JxbWL-PT"
   },
   "source": [
    "Transformando os valores do conjunto de dados em _one hot enconding_ para definição dos gêneros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UvQVt4SL-PU"
   },
   "outputs": [],
   "source": [
    "features_imagem = []\n",
    "\n",
    "faces, idades, generos, faces_ages = carregar_imagens_faces()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1sn8WPaEy5Ed"
   },
   "outputs": [],
   "source": [
    "features_imagem = np.array(faces)\n",
    "print(features_imagem.shape)\n",
    "features_imagem = features_imagem.reshape(features_imagem.shape[0], 224, 224, 3)\n",
    "print(features_imagem.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6ORPcOTL-PW"
   },
   "source": [
    "Liberar memória após o uso das imagens serem padronizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Hy_ODy3L-PW"
   },
   "outputs": [],
   "source": [
    "faces = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ThjVmf-YL-PY"
   },
   "outputs": [],
   "source": [
    "generos_classes = keras.utils.to_categorical(generos, num_classes_genero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u4r_9b-L-Pa"
   },
   "source": [
    "Como para ambos os modelos, de idade e gênero, utilizamos a mesma base de _transfer learning_ do VGGFace, vamos reutilizá-la também neste modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9uLf8TiL-Pb"
   },
   "outputs": [],
   "source": [
    "for layer in modelo.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "saida_modelo = Sequential()\n",
    "saida_modelo = Convolution2D(num_classes_genero, (1, 1), name='predictions')(modelo.layers[-4].output)\n",
    "saida_modelo = Flatten()(saida_modelo)\n",
    "saida_modelo = Activation('softmax')(saida_modelo)\n",
    "\n",
    "modelo_genero = Model(inputs=modelo.input, outputs=saida_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24wd30gRL-Pd"
   },
   "source": [
    "### 3.2 Separação de dados de treinamento e validação\n",
    "\n",
    "É recomendavel manter a mesma porcentagem do modelo anterior para dividir os dados de treinamento e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldUFNR1PL-Pd"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTAR\n",
    "porcentagem_validacao = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLRHI3t2L-Pg"
   },
   "outputs": [],
   "source": [
    "treinamento_x, teste_x, treinamento_y, teste_y = train_test_split(features_imagem, \n",
    "                                                                  generos_classes, \n",
    "                                                                  test_size=porcentagem_validacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eS9edmJL-Pj"
   },
   "source": [
    "Liberando memória da variável ```features_imagem``` que não será utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obXn4AtwL-Pj"
   },
   "outputs": [],
   "source": [
    "features_imagem = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUUqj9YQL-Pl"
   },
   "source": [
    "### 3.3 Treinamento e validação\n",
    "\n",
    "A seguir iremos treinar o modelo para classificação de gêneros.\n",
    "De forma semelhante realizada anteriormente, é necessário definir o número de épocas para esta etapa. Recomendamos um valor mínimo de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofQROYhzL-Pl"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "numero_epocas = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TamZ8GGUL-Pn"
   },
   "source": [
    "*Caso o modelo trave no treinamento, experimente diminuir o ```batch_size```*.\n",
    "\n",
    "Este modelo é mais simples por ter menos classes, logo podemos assumir um ```batch_size``` maior que o anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sE4LKAoUL-Pn"
   },
   "outputs": [],
   "source": [
    "modelo_genero.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "historico_genero = modelo_genero.fit(x=treinamento_x, y=treinamento_y, \n",
    "                             validation_data=(teste_x, teste_y), epochs=numero_epocas, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4hwUBK0L-Pp"
   },
   "source": [
    "Verifique abaixo, por meio dos gráficos, como está a evolução da função de erro e acurácia do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z0nZz1MxL-Pq"
   },
   "outputs": [],
   "source": [
    "# Para deixar no formato do Seaborn os gráficos do Pyplot\n",
    "sns.set()\n",
    "\n",
    "acc_var = 'acc'\n",
    "valacc_var = 'val_acc'\n",
    "# in some versions is 'accuracy' or 'acc'\n",
    "if 'accuracy' in historico.history:\n",
    "    acc_var = 'accuracy'\n",
    "    valacc_var = 'val_accuracy'\n",
    "\n",
    "# Exibindo dados de Acurácia/Precisão\n",
    "plt.plot(historico_genero.history[acc_var])\n",
    "plt.plot(historico_genero.history[valacc_var])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Exibindo dados de Perda\n",
    "plt.plot(historico_genero.history['loss'])\n",
    "plt.plot(historico_genero.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XfRUK1sCL-Ps"
   },
   "source": [
    "**Pergunta:** Qual foi a tendência da função de erro e acurácia do modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Hqt7OeHL-Ps"
   },
   "source": [
    "**Resposta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_7vfZ_8L-Pt"
   },
   "source": [
    "Salve os modelos abaixo para uso em inferências mais adiante no projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H60kaLOvL-Pt"
   },
   "outputs": [],
   "source": [
    "# Salvando o modelo no formato HDf5\n",
    "modelo_genero.save(\"modelos/modelo_genero.h5\")\n",
    "modelo_genero.save_weights(\"pesos/modelo_genero_pesos.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCyupee3L-Pu"
   },
   "source": [
    "Os comandos a seguir são para carregar os pesos e o modelo previamente treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvYDZY2OL-Px"
   },
   "outputs": [],
   "source": [
    "# carregando o modelo no formato HDf5\n",
    "modelo_genero = load_model(\"modelos/modelo_genero.h5\")  \n",
    "modelo_genero.load_weights(\"pesos/modelo_genero_pesos.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4l3ntK6L-Pz"
   },
   "source": [
    "> **Critério de sucesso:** acurácia do modelo, com base nas amostra de teste, superior a 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUiVFW76L-Pz"
   },
   "source": [
    "### 3.4 Avaliação final\n",
    "\n",
    "Com base em algumas amostras da base de testes, vamos inferir 6 imagens para verificarmos o desempenho do modeo.\n",
    "Selecionamos 3 imagens de homens e mulheres.\n",
    "\n",
    "Como este modelo é mais simples, ou seja, o resultado sempre vai ser a classe com maior probabilidade, não precisamos realizar a somatória dos pesos individualmente como fizemos no modelo anterior.\n",
    "\n",
    "Assim o resultado final será obtido pela maior classe. A função ```np.argmax``` retorna a maior classe da predição realizada pelo modelo.\n",
    "\n",
    "De acordo com o _encoding_, o índice 0 indica gênero masculino e o índice 1 o gênero feminino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UudUcv-L-P0"
   },
   "outputs": [],
   "source": [
    "predicao_1 = modelo_genero.predict(imagem_1_pad)\n",
    "resultado_1 = \"Masculino\" if np.argmax(predicao_1) == 1 else \"Feminino\"\n",
    "\n",
    "predicao_2 = modelo_genero.predict(imagem_2_pad)\n",
    "resultado_2 = \"Masculino\" if np.argmax(predicao_2) == 1 else \"Feminino\"\n",
    "\n",
    "predicao_3 = modelo_genero.predict(imagem_3_pad)\n",
    "resultado_3 = \"Masculino\" if np.argmax(predicao_3) == 1 else \"Feminino\"\n",
    "\n",
    "predicao_4 = modelo_genero.predict(imagem_4_pad)\n",
    "resultado_4 = \"Masculino\" if np.argmax(predicao_4) == 1 else \"Feminino\"\n",
    "\n",
    "predicao_5 = modelo_genero.predict(imagem_5_pad)\n",
    "resultado_5 = \"Masculino\" if np.argmax(predicao_5) == 1 else \"Feminino\"\n",
    "\n",
    "predicao_6 = modelo_genero.predict(imagem_6_pad)\n",
    "resultado_6 = \"Masculino\" if np.argmax(predicao_6) == 1 else \"Feminino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLgckw2ML-P3"
   },
   "source": [
    "Agora vamos exibir no gráfico as imagens e as inferências realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xU3QemdXL-P3"
   },
   "outputs": [],
   "source": [
    "# Para deixar no formato do Seaborn os gráficos do Pyplot\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(231)\n",
    "plt.title(\"Gênero inferido: \" + resultado_1)\n",
    "plt.imshow(imagem_1)\n",
    "plt.subplot(232)\n",
    "plt.title(\"Gênero inferido: \" + resultado_2)\n",
    "plt.imshow(imagem_2)\n",
    "plt.subplot(233)\n",
    "plt.title(\"Gênero inferido: \" + resultado_3)\n",
    "plt.imshow(imagem_3)\n",
    "plt.subplot(234)\n",
    "plt.title(\"Gênero inferido: \" + resultado_4)\n",
    "plt.imshow(imagem_4)\n",
    "plt.subplot(235)\n",
    "plt.title(\"Gênero inferido: \" + resultado_5)\n",
    "plt.imshow(imagem_5)\n",
    "plt.subplot(236)\n",
    "plt.title(\"Gênero inferido: \" + resultado_6)\n",
    "plt.imshow(imagem_6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXReEW3tL-P6"
   },
   "source": [
    "Vamos aproveitar para deixar preparado a função de predição de gênero para usarmos mais adiante.\n",
    "Implemente a função abaixo para retornar \"Masculino\" se a classe com maior probabilidade for 1, ou \"Feminino\" se a maior probabilidade for a classe igual a 0.\n",
    "É do mesmo jeito que fizemos anteriormente na predição de idade. Assuma que o parâmetro de entrada seja uma imagem já padronizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ny7HosGnL-P7"
   },
   "outputs": [],
   "source": [
    "def predizerGenero(imagem):\n",
    "    # IMPLEMENTAR\n",
    "    predicao = modelo_genero.predict(imagem)\n",
    "    return \"Masculino\" if np.argmax(predicao) == 1 else \"Feminino\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVDX10mjL-P_"
   },
   "source": [
    "Liberando memória de variáveis sem utilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHVKWHaiL-P_"
   },
   "outputs": [],
   "source": [
    "treinamento_x, teste_x, treinamento_y, teste_y  = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksMctzU2L-QE"
   },
   "source": [
    "## 4. Identificação de faces\n",
    "\n",
    "Nesta etapa precisamos construir uma forma de extrair regiões de interesse de imagens que sejam rostos. \n",
    "Problema de reconhecimento de faces aprofundado é realizada a partir de um recorte de região de interesse. Logo, nossa estratégia agora é como extrair uma ou mais faces de uma imagem para que seja possível, posteriormente, aplicarmos os modelos que acabamos de classificar.\n",
    "\n",
    "Vamos começar com um teste simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hl3OGgZbL-QE"
   },
   "outputs": [],
   "source": [
    "imagem = cv2.imread('testes/teste-8.png')\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMBNVdHFL-QG"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgyuSbXsL-QL"
   },
   "source": [
    "Escolha uma forma de identificar rostos de uma imagem.\n",
    "Recomendamos as seguintes formas:\n",
    "\n",
    "* Classificador em cascata de Haar\n",
    "* Detector de face baseado em Hog (DLib), recomendamos utilizar o classificador de 68 pontos para maior precisão\n",
    "\n",
    "Dada a imagem de testes apresentada, execute seu algoritmo escolhido e retorne o número de faces na variável ```faces```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLgtN62jL-QM"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "frame_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "frame_gray = cv2.equalizeHist(frame_gray)\n",
    "\n",
    "frame = imagem\n",
    "hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "faces = hogFaceDetector(imagem, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SlKMLduL-QO"
   },
   "outputs": [],
   "source": [
    "print(\"Faces encontradas: \" + str(len(faces)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFVfE4prL-QP"
   },
   "source": [
    "> **Critério de sucesso:** encontrar as 2 faces da imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Wjm9AoCL-QP"
   },
   "source": [
    "Com as faces identificadas, desenhe um retângulo em cada uma delas.\n",
    "Para desenhar um retângulo, é necessário os seguintes dados: coordenada x, y, comprimento (w) e altura (h).\n",
    "\n",
    "Caso utilize o detector de faces do DLib, o valor da coordenada x é equivalente ao atributo _left_ e a coordenada y é o equivalente ao atributo _top_.\n",
    "\n",
    "O DLib é um identificador tão versátil que é capaz de identificar rostos parciais em uma imagem. No entanto, quando isso ocorre as coordenadas podem ser negativas, o que pode demandar um certo ajuste para obter o valor da região de interesse, por exemplo igualando a coordenada a 0.\n",
    "\n",
    "Utilize o seguinte comando para desenhar um retângulo em cada face identificada.\n",
    "\n",
    "````\n",
    "cv2.rectangle(imagem_anotada, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VBnmAkXL-QQ"
   },
   "source": [
    "Criando uma cópia da imagem original para anotação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eixW_gt4L-QQ"
   },
   "outputs": [],
   "source": [
    "imagem_anotada = imagem.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rg3ScVlL-QS"
   },
   "outputs": [],
   "source": [
    "#IMPLEMENTAR\n",
    "for faceRect in faces:\n",
    "    x1 = faceRect.left()\n",
    "    y1 = faceRect.top()\n",
    "    x2 = faceRect.right()\n",
    "    y2 = faceRect.bottom()\n",
    "    imagem_anotada = cv2.rectangle(imagem_anotada, (x1, y1), (x2, y2), (255,255,0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWAiHzT8L-QU"
   },
   "source": [
    "Exibindo a imagem com os retângulos desenhados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lwGZTdoL-QU"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem_anotada)\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYsWnK2JL-QX"
   },
   "source": [
    "Uma vez que já foi definido o melhor algoritmo de identificação de faces, precisamos construir uma função que posteriormente utilizaremos no processo de processamento de vídeos.\n",
    "\n",
    "Essa função deverá receber na entrada uma imagem colorida e retornar, na forma de lista, um dicionário com as seguintes informações: extração do ROI do rosto e as coordenadas da localização do rosto.\n",
    "\n",
    "Exemplo de retorno:\n",
    "\n",
    "```json\n",
    "[{'coordenadas': [array([589, 290, 386, 386])],\n",
    "  'rosto': array([[[38, 21, 19],\n",
    "          [40, 21, 20],\n",
    "          [41, 22, 20],\n",
    "          ...,\n",
    "          [36, 19, 13],\n",
    "          [37, 20, 15],\n",
    "          [40, 23, 18]],...}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUnrGwScL-QX"
   },
   "source": [
    "O atributo rosto utilizaremos para aplicarmos os modelos de reconhecimento de idade e gênero, e as coordenadas serão utilizadas para fazer os recortes nas imagens.\n",
    "\n",
    "Com isso, a chave ```rosto``` deverá ter o _slice_ da imagem neste padrão ```imagem[y:y+h, x:x+w]``` e a chave ```coordenadas``` deverá ter a saída dos pontos (x, y, h, w) neste padrão ```[np.array(lista_coordenadas)]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLVmULlyL-QX"
   },
   "outputs": [],
   "source": [
    "def obterFaces(imagem):\n",
    "    rostos = []\n",
    "\n",
    "    #IMPLEMENTAR\n",
    "    hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "    faces = hogFaceDetector(imagem, 0)\n",
    "\n",
    "    for faceRect in faces:\n",
    "      rosto = {}\n",
    "      x = faceRect.left()\n",
    "      y = faceRect.top()\n",
    "      w = faceRect.right() - x\n",
    "      h = faceRect.bottom() - y\n",
    "      if(x > 0 and y > 0 and w > 0 and h > 0):\n",
    "        rosto['coordenadas'] = np.array([x, y, w, h] , dtype=np.int32)\n",
    "        rosto['rosto'] = imagem[y:y+h, x:x+w]\n",
    "        rostos.append(rosto)\n",
    "\n",
    "    return rostos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Jj-311yL-QZ"
   },
   "source": [
    "Vamos aplicar um teste com a imagem anterior (variável ```imagem```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4Li2flJL-QZ"
   },
   "outputs": [],
   "source": [
    "rostos = obterFaces(imagem)\n",
    "rostos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkHt_XetL-Qb"
   },
   "source": [
    "> **Critério de sucesso:** retornar 2 valores (da lista) com coordenadas e rosto para cada face identificada. Exemplo:\n",
    "\n",
    "\n",
    "```\n",
    "[{'coordenadas': [array([534, 20,  90, 100], dtype=int32)],\n",
    "  'rosto': array([...], shape=(0, 89, 3), dtype=uint8)},\n",
    "  {'coordenadas': [array([222, 13,  90, 100], dtype=int32)],\n",
    "  'rosto': array([...], shape=(0, 89, 3), dtype=uint8)}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uh8KnSPsL-Qb"
   },
   "source": [
    "Agora vamos testar se os parâmetros estão funcionando adequadamente.\n",
    "\n",
    "Primeiro a imagem do rosto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkGtLAH3L-Qb"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(rostos[0][\"rosto\"])\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9qh6UZeL-Qd"
   },
   "source": [
    "O próximo passo são as coordenadas. Para simplificar, vamos desenhar um retângulo na imagem original com estes pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FEuFlxpLL-Qd"
   },
   "outputs": [],
   "source": [
    "imagem_anotada = imagem.copy()\n",
    "\n",
    "for rosto_item in rostos:\n",
    "  (x,y,w,h) = rosto_item[\"coordenadas\"]\n",
    "  cv2.rectangle(imagem_anotada, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem_anotada)\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HgxaRdsL-Qf"
   },
   "source": [
    "### 4.1 Padronização de tamanho de imagem\n",
    "\n",
    "Adiante, quando aplicarmos a classificação dos rostos, precisaremos antes padronizar a imagem para o tamanho que o modelo recebe na camada de entrada bem como a normalização dos pixels da imagem, ou seja, ao invés de estar no formato de intensidade de pixel de 0 a 255, deve estar representado entre 0 e 1.\n",
    "\n",
    "As implementações deverão ser as seguintes:\n",
    "\n",
    "1. ```IMPLEMENTAR 1```: redimensionar a imagem para o tamanho da camada de entrada. Utilize uma interpolação adequada para não perder qualidade de imagem.\n",
    "2. ```IMPLEMENTAR 2```: normalizar a intensidade de pixel da imagem para 0 e 1 e não 0 a 255, que é o padrão.\n",
    "\n",
    "As demais instruções seguem inalteradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr1nae2jL-Qf"
   },
   "outputs": [],
   "source": [
    "def padronizarROI(imagem):\n",
    "    # IMPLEMENTAR 1\n",
    "    imagem = cv2.resize(imagem, (224, 224), interpolation = cv2.INTER_CUBIC)\n",
    "    #image_array = image.img_to_array(imagem).reshape(1, -1)[0]\n",
    "    #image_array /= 255\n",
    "    \n",
    "    imagem = image.img_to_array(imagem)\n",
    "    imagem = np.expand_dims(imagem, axis = 0)\n",
    "    \n",
    "    # IMPLEMENTAR 2\n",
    "    imagem = imagem.astype('float32')\n",
    "    imagem = imagem/255.0\n",
    "    \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "yK8l6eMFL-Qh"
   },
   "source": [
    "Agora, vamos avançar para a identificação de objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMGus8qPL-Qi"
   },
   "source": [
    "## 5. Identificação de objetos\n",
    "\n",
    "Além de identificarmos as pessoas, precisamos também identificar diferentes objetos.\n",
    "Uma forma de alcançarmos tal objetivo é utilizar um modelo já treinado com diversos objetos treinados.\n",
    "\n",
    "O modolo Yolov3, por exemplo, possui 80 diferentes objetos em seu modelo, servindo muito bem para o propósito do desafio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "KrEkIHigL-Ql"
   },
   "source": [
    "### 5.1 Configurando o modelo Yolov3\n",
    "\n",
    "Primeiramente baixe os pesos diretamente no site do Darknet, neste [link](https://pjreddie.com/media/files/yolov3.weights). Copie o arquivo ```yolov3.weights``` para a pasta pesos.\n",
    "Confira o arquivo ```yolo-classes/coco.anmes```. Estas são as classes de todos os objetos que são possíveis identificar.\n",
    "Por fim, verifique se o arquivo ```config/yolov3.cfg``` está presente. Não é necessário ajustar nenhum parâmetro nele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqQyQDcuL-Ql"
   },
   "outputs": [],
   "source": [
    "# Carregar os labels do conjunto de dados Coco\n",
    "label_path = \"yolo-classes/coco.names\"\n",
    "labels = open(label_path).read().strip().split(\"\\n\")\n",
    "\n",
    "# Atribuir a cada label uma cor diferente (randômica)\n",
    "np.random.seed(42)\n",
    "cores = np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")\n",
    "\n",
    "# Definir caminho dos arquivos de pesos e configuração\n",
    "pesos_path =\"pesos/yolov3.weights\"\n",
    "config_path = \"config/yolov3.cfg\"\n",
    "\n",
    "# Carregar a rede\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, pesos_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kc3b5D8CL-Qn"
   },
   "source": [
    "Se o código carregou as configurações sem erros, estamos prontos para avançar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCUH2tNTL-Qn"
   },
   "source": [
    "Os valores de confiança e supressão não máxima são atributes importantes para o processo de detecção de objetos. Escolha valores adequados que permitam a detecção aceitável e com valores próximos os critérios de sucesso apresentandos mais adiante.\n",
    "\n",
    "Os valores precisam ser numéricos fracionários. Sendo que 1 é igual a 100%. Valores médios costumam apresentar performance razoável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DprHtSK8L-Qn"
   },
   "outputs": [],
   "source": [
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccPjn7AcL-Qq"
   },
   "source": [
    "**Pergunta:** Qual é a influência do parâmetro de confiança e supressão não máxima na performance do modelo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "va81HjhUL-Qq"
   },
   "source": [
    "**Resposta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2JUviSZL-Qq"
   },
   "source": [
    "### 5.2 Obtendo região de interesse e coordenadas\n",
    "\n",
    "De forma semelhante que foi feito na identificação de faces, vamos fazer para a identificação de objetos.\n",
    "Na função abaixo vamos implementar um algoritmo que retornará em um dicionário os valores do recorte da imagem, o que chamamos de região de interesse e suas coordenadas x, y, w e h.\n",
    "\n",
    "A entrada da função é uma imagem colorida e uma lista de objetos para identificar, no formato de lista.\n",
    "Sua saída, é uma lista de objetos no seguinte formato:\n",
    "\n",
    "```json\n",
    "[{'coordenadas': [array([589, 290, 386, 386])],\n",
    "  'objeto': array([[[38, 21, 19],\n",
    "          [40, 21, 20],\n",
    "          [41, 22, 20],\n",
    "          ...,\n",
    "          [36, 19, 13],\n",
    "          [37, 20, 15],\n",
    "          [40, 23, 18]],...}]\n",
    "```\n",
    "\n",
    "Onde ```coordenadas``` é uma lista das coordenadas x, y, w e h no formato array, igual ao que foi apresentado anteriormente: ```[np.array(lista_coordenadas)]```. E ```objeto``` é a região de interesse do objeto extraído, igual ao que foi apresentado anteriormente: ```imagem[y:y+h, x:x+w]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EV5FYhYL-Qr"
   },
   "outputs": [],
   "source": [
    "def obter_objetos(imagem, lista_objetos):\n",
    "    \n",
    "    (H, W) = imagem.shape[:2]\n",
    "\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(imagem, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    objetos = []\n",
    "\n",
    "    for output in layerOutputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            if confidence > conf_threshold:\n",
    "                \n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                x = int(centerX - (width/2))\n",
    "                y = int(centerY - (height/2))\n",
    "\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "    if len(idxs) > 0:\n",
    "        for i in idxs.flatten():\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            \n",
    "            if x < 0:\n",
    "                x = 0\n",
    "            if y < 0:\n",
    "                y = 0\n",
    "\n",
    "            text = \"{}: {:.4f}\".format(labels[classIDs[i]], confidences[i])\n",
    "            \n",
    "            if labels[classIDs[i]] in lista_objetos:\n",
    "                print(\"Identificado \" + text)\n",
    "                \n",
    "                # IMPLEMENTAR\n",
    "                item = {\"objeto\": imagem[y:y+h, x:x+w], \"coordenadas\": np.array([x, y, w, h], dtype=np.int32), \"nome\": labels[classIDs[i]]}\n",
    "                \n",
    "                objetos.append(item)\n",
    "\n",
    "    return objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGnA_EnXL-Qt"
   },
   "source": [
    "Vamos utilizar outra imagem de testes para validar o algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-39MfewL-Qt"
   },
   "outputs": [],
   "source": [
    "imagem_inferencia = cv2.imread(\"testes/teste-9.png\")\n",
    "imagem_inferencia = cv2.cvtColor(imagem_inferencia, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BR7JvTQYL-Qu"
   },
   "source": [
    "Na variável ```lista_objetos``` preecha com uma lista de Strings com os valores \"pessoa\" e \"gravata\" que são os objetos a serem identificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrKiW1FoL-Qv"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "lista_objetos = [\"pessoa\", \"gravata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vTJdC9dL-Qw"
   },
   "source": [
    "Execute a função com a lista definida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hilju-aqL-Qw"
   },
   "outputs": [],
   "source": [
    "objetos = obter_objetos(imagem_inferencia, lista_objetos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jc4sU46Hsvlx"
   },
   "outputs": [],
   "source": [
    "objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbIJpYM9L-Qy"
   },
   "source": [
    "Para nos certificamos que a identificação está correta, vamos desenhar um retângulo delimitador na imagem e verificar como foi a identificação.\n",
    "\n",
    "Neste caso vamos utilizar o parâmetro ```coordenadas``` do retorno da função para cada objeto identificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKTleE1GL-Qy"
   },
   "outputs": [],
   "source": [
    "imagem_anotada = imagem_inferencia.copy()\n",
    "for obj in objetos:\n",
    "    (x,y,w,h) = obj[\"coordenadas\"]\n",
    "    cv2.rectangle(imagem_anotada, (x,y), (x+w,y+h), (255,255,0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtWhve_1L-Qz"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem_anotada)\n",
    "plt.title(\"Imagem Inferida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-YsuAHFL-Q2"
   },
   "source": [
    "Vamos dar uma olhada em 3 regiões de interesse detectadas.\n",
    "Lembrando que estas regiões são imagens, podemos renderizá-las diretamente no Pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcDJHpNLL-Q3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(objetos[0][\"objeto\"])\n",
    "plt.title(\"Região de interesse #1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jl3sS-GL-Q4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(objetos[1][\"objeto\"])\n",
    "plt.title(\"Região de interesse #1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wECnZzDlL-Q5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(objetos[2][\"objeto\"])\n",
    "plt.title(\"Região de interesse #1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VuWOI0v2L-Q7"
   },
   "source": [
    "Perfeito, agora já estamos prontos para juntar todas as peças e começar nossa auditoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lR18BcnQL-Q7"
   },
   "source": [
    "## 6. Auditoria automática em vídeo\n",
    "\n",
    "Esta é a parte final do projeto.\n",
    "Vamos fazer um checkpoint até aqui para ter certeza de que fizemos com sucesso os passos anteriores.\n",
    "\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de idades treinado\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de idades salvo\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de idades carregado\n",
    "* <input type=\"checkbox\" disabled checked> Função de detecção de idades\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de gênero treinado\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de gênero salvo\n",
    "* <input type=\"checkbox\" disabled checked> Modelo de detecção de gênero carregado\n",
    "* <input type=\"checkbox\" disabled checked> Função de detecção de gênero\n",
    "* <input type=\"checkbox\" disabled checked> Função de padronização de imagens\n",
    "* <input type=\"checkbox\" disabled checked> Função de extração de faces\n",
    "* <input type=\"checkbox\" disabled checked> Função de extração de objetos\n",
    "\n",
    "O que precisamos identificar no vídeo de auditoria:\n",
    "\n",
    "* Homens com mais de 45 anos\n",
    "* Mulheres com menos de 45 anos\n",
    "* Objetos _computador portátil, celular, teclado, tv _ e _controle remoto_.\n",
    "\n",
    "Vamos começar definindo a lista de objetos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwDSKFeML-Q7"
   },
   "outputs": [],
   "source": [
    "lista_objetos = [\"computador portátil\", \"celular\", \"teclado\", \"tv\", \"controle remoto\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqJXurLeL-Q9"
   },
   "source": [
    "Nesta parte vamos reunir tudo o que fizemos até aqui.\n",
    "O algoritmo irá abrir um vídeo e, frame a frame, analisar seu conteúdo.\n",
    "Após a instrução ```if is_capturing``` iremos implementar as verificações.\n",
    "\n",
    "A execução abaixo pode demorar, pois será analisado individualmente cada frame do vídeo.\n",
    "\n",
    "O que precisamos fazer:\n",
    "\n",
    "* ```IMPLEMENTAR 1```: obter as faces de uma imagem. Neste caso receberamos uma lista de rostos no padrão que já vimos, ou seja, uma lista de ```rosto``` e ```coordenadas```.\n",
    "\n",
    "* ```IMPLEMENTAR 2```: obter as a face de um item e padronizar. Lembre-se de utilizar a função de padronização que foi desenvolvida anteriormente, que tem por finalidade ajustar o tamanho da imagem e também normalizá-la.\n",
    "\n",
    "* ```IMPLEMENTAR 3```: executar as funções de identificar a idade e gênero para a tomada de decisões de coleta de evidências.\n",
    "\n",
    "* ```IMPLEMENTAR 4```: criar as regras de armazenamento de evidências para pessoas. Consulte as regras no início do projeto sobre quais os gêneros e idades de interesse. As regiões de interesse nesse caso, os rostos, deverão ser salvos individualmente na pasta ```resultado/homem``` e ```resultado/mulher```.\n",
    "\n",
    "* ```IMPLEMENTAR 5```: criar as regras de armazenamento de evidências para objetos. As regiões de interesse nesse caso, os objetos, deverão ser salvos individualmente na pasta ```resultado/objetos```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob('resultado/homem/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "files = glob.glob('resultado/mulher/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "files = glob.glob('resultado/objetos/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7U2rwFvuz-z"
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(\"videos/video-1.avi\")\n",
    "frame_count = int(cam.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "cam.release()\n",
    "print(\"Total frames: \"+ str(frame_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ho_2KA7xL-Q9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "#cam.release()\n",
    "cam = cv2.VideoCapture(\"videos/video-1.avi\")\n",
    "contador = 0\n",
    "frame = 0\n",
    "\n",
    "try:\n",
    "    while(True):\n",
    "        frame += 1\n",
    "        is_capturing, imagem = cam.read()\n",
    "        print(\"Frame [\"+str(frame)+\"/\"+str(frame_count)+\"]\")\n",
    "        \n",
    "        if is_capturing:\n",
    "            \n",
    "            # IMPLEMENTAR 1\n",
    "            # Obter Faces\n",
    "            faces = obterFaces(imagem)\n",
    "            print(\"Encontrado \" + str(len(faces)) + \" rostos...\")\n",
    "            \n",
    "            for idx, face in enumerate(faces):\n",
    "                \n",
    "                # IMPLEMENTAR 2\n",
    "                # Padronizar a imagem do rosto (ROI)\n",
    "                # Obtenha a imagem do rosto da variável face e armazene em imagem_rosto\n",
    "                # Depois utilize a função padronizarROI, com a variável imagem_rosto para obter\n",
    "                # o rosto padronizado e armazenar em rosto_padronizado\n",
    "                \n",
    "                imagem_rosto = faces[idx]['rosto']\n",
    "                #print(\"imagem_rosto.shape \" + str(imagem_rosto.shape))\n",
    "                rosto_padronizado = padronizarROI(imagem_rosto)\n",
    "                rosto_padronizado = rosto_padronizado.reshape(1, 224, 224, 3)\n",
    "                #print(\"rosto_padronizado.shape \" + str(rosto_padronizado.shape))\n",
    "                \n",
    "                # IMPLEMENTAR 3\n",
    "                # Chame as funções para predizer gênero e idade com a imagem padronizada do rosto\n",
    "                genero = predizerGenero(rosto_padronizado)\n",
    "                idade = predizerIdade(rosto_padronizado)\n",
    "                idade  = int(idade)\n",
    "                \n",
    "                print(\"Gênero: \" + genero + \", idade: \" + str(idade))\n",
    "                \n",
    "                # IMPLEMENTAR 4\n",
    "                # Estabeleça as regras de auditoria e salve as evidências (imagens) no diretório resultados\n",
    "                # de acordo com o identificação (resultado/homem, resultado/mulher)\n",
    "                # Cuidado para não sobrescrever as imagens\n",
    "                if(idade > 45):\n",
    "                    if (genero == \"Masculino\"):\n",
    "                        cv2.imwrite(\"resultado/homem/\"+str(contador)+\"_\"+str(idade)+\".png\", imagem_rosto)\n",
    "                    else:\n",
    "                        cv2.imwrite(\"resultado/mulher/\"+str(contador)+\"_\"+str(idade)+\".png\", imagem_rosto)\n",
    "                    contador += 1\n",
    "                    \n",
    "            objetos = obter_objetos(imagem, lista_objetos)\n",
    "\n",
    "            # IMPLEMENTAR 5\n",
    "            # Estabeleça as regras de auditoria e salve as evidências (imagens) no diretório resultados\n",
    "            # de acordo com o identificação (resultado/objetos)\n",
    "            # Cuidado para não sobrescrever as imagens\n",
    "            for obj in objetos:\n",
    "                cv2.imwrite(\"resultado/objetos/\"+obj[\"nome\"]+str(contador)+\"_\"+\".png\", obj[\"objeto\"])\n",
    "                contador += 1\n",
    "\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    cam.release()\n",
    "    print(\"Interrompido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPP0GYBY9Rqq"
   },
   "outputs": [],
   "source": [
    "!ls -1 \"resultado/homem\" | wc -l\n",
    "!ls -1 \"resultado/mulher\" | wc -l\n",
    "!ls -1 \"resultado/objetos\" | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "zudJtSqOL-RA"
   },
   "source": [
    "> **Critério de sucesso:** identificação de 180 imagens de homens. Como uma mesma pessoa aparece em diferentes frames é comum repetir as imagens.\n",
    "\n",
    "> **Critério de sucesso:** identificação de 190 imagens de mulheres. Como uma mesma pessoa aparece em diferentes frames é comum repetir as imagens.\n",
    "\n",
    "> **Critério de sucesso:** identificação de 680 imagens de objetos selecionados. Como um mesmo objeto pode aparecer em diferentes frames é comum repetir as imagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NfNM5QrL-RA"
   },
   "source": [
    "## 7. Conclusões finais\n",
    "\n",
    "Com base nesta jornada de construção de modelos, análises de regiões de interesse e processamento de vídeo, comente quais seriam os principais pontos de melhoria para alcançar resultados melhores em todas as etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fd1smfzwL-RD"
   },
   "source": [
    "**Resposta:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "object-people-audit-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
